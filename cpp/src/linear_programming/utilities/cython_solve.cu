/* clang-format off */
/*
 * SPDX-FileCopyrightText: Copyright (c) 2023-2026, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
 * SPDX-License-Identifier: Apache-2.0
 */
/* clang-format on */

#include <cuopt/error.hpp>
#include <cuopt/linear_programming/cpu_optimization_problem_solution.hpp>
#include <cuopt/linear_programming/gpu_optimization_problem_solution.hpp>
#include <cuopt/linear_programming/optimization_problem.hpp>
#include <cuopt/linear_programming/optimization_problem_interface.hpp>
#include <cuopt/linear_programming/optimization_problem_utils.hpp>
#include <cuopt/linear_programming/solve.hpp>
#include <cuopt/linear_programming/solver_settings.hpp>
#include <cuopt/linear_programming/utilities/cython_solve.hpp>
#include <mip/logger.hpp>
#include <mps_parser/data_model_view.hpp>
#include <mps_parser/mps_data_model.hpp>
#include <mps_parser/writer.hpp>
#include <utilities/copy_helpers.hpp>

#include <raft/common/nvtx.hpp>
#include <raft/core/handle.hpp>

#include <rmm/device_buffer.hpp>

#include <utility>
#include <vector>

#include <unistd.h>

namespace cuopt {
namespace cython {

using cuopt::linear_programming::var_t;

static cuopt::linear_programming::optimization_problem_t<int, double>
data_model_to_optimization_problem(
  cuopt::mps_parser::data_model_view_t<int, double>* data_model,
  cuopt::linear_programming::solver_settings_t<int, double>* solver_settings,
  raft::handle_t const* handle_ptr)
{
  cuopt::linear_programming::optimization_problem_t<int, double> op_problem(handle_ptr);
  op_problem.set_maximize(data_model->get_sense());
  if (data_model->get_constraint_matrix_values().size() != 0 &&
      data_model->get_constraint_matrix_indices().size() != 0 &&
      data_model->get_constraint_matrix_offsets().size() != 0) {
    op_problem.set_csr_constraint_matrix(data_model->get_constraint_matrix_values().data(),
                                         data_model->get_constraint_matrix_values().size(),
                                         data_model->get_constraint_matrix_indices().data(),
                                         data_model->get_constraint_matrix_indices().size(),
                                         data_model->get_constraint_matrix_offsets().data(),
                                         data_model->get_constraint_matrix_offsets().size());
  }
  if (data_model->get_constraint_bounds().size() != 0) {
    op_problem.set_constraint_bounds(data_model->get_constraint_bounds().data(),
                                     data_model->get_constraint_bounds().size());
  }
  if (data_model->get_objective_coefficients().size() != 0) {
    op_problem.set_objective_coefficients(data_model->get_objective_coefficients().data(),
                                          data_model->get_objective_coefficients().size());
  }
  op_problem.set_objective_scaling_factor(data_model->get_objective_scaling_factor());
  op_problem.set_objective_offset(data_model->get_objective_offset());

  if (data_model->get_quadratic_objective_values().size() != 0 &&
      data_model->get_quadratic_objective_indices().size() != 0 &&
      data_model->get_quadratic_objective_offsets().size() != 0) {
    op_problem.set_quadratic_objective_matrix(data_model->get_quadratic_objective_values().data(),
                                              data_model->get_quadratic_objective_values().size(),
                                              data_model->get_quadratic_objective_indices().data(),
                                              data_model->get_quadratic_objective_indices().size(),
                                              data_model->get_quadratic_objective_offsets().data(),
                                              data_model->get_quadratic_objective_offsets().size());
  }
  if (data_model->get_variable_lower_bounds().size() != 0) {
    op_problem.set_variable_lower_bounds(data_model->get_variable_lower_bounds().data(),
                                         data_model->get_variable_lower_bounds().size());
  }
  if (data_model->get_variable_upper_bounds().size() != 0) {
    op_problem.set_variable_upper_bounds(data_model->get_variable_upper_bounds().data(),
                                         data_model->get_variable_upper_bounds().size());
  }

  if (data_model->get_row_types().size() != 0) {
    op_problem.set_row_types(data_model->get_row_types().data(),
                             data_model->get_row_types().size());
  }
  if (data_model->get_constraint_lower_bounds().size() != 0) {
    op_problem.set_constraint_lower_bounds(data_model->get_constraint_lower_bounds().data(),
                                           data_model->get_constraint_lower_bounds().size());
  }
  if (data_model->get_constraint_upper_bounds().size() != 0) {
    op_problem.set_constraint_upper_bounds(data_model->get_constraint_upper_bounds().data(),
                                           data_model->get_constraint_upper_bounds().size());
  }

  if (solver_settings->get_pdlp_warm_start_data_view()
        .last_restart_duality_gap_dual_solution_.data() != nullptr) {
    // Moved inside
    cuopt::linear_programming::pdlp_warm_start_data_t<int, double> pdlp_warm_start_data(
      solver_settings->get_pdlp_warm_start_data_view(), handle_ptr->get_stream());
    solver_settings->get_pdlp_settings().set_pdlp_warm_start_data(pdlp_warm_start_data);
  }

  if (data_model->get_variable_types().size() != 0) {
    std::vector<var_t> enum_variable_types(data_model->get_variable_types().size());
    std::transform(
      data_model->get_variable_types().data(),
      data_model->get_variable_types().data() + data_model->get_variable_types().size(),
      enum_variable_types.begin(),
      [](const auto val) -> var_t { return val == 'I' ? var_t::INTEGER : var_t::CONTINUOUS; });
    op_problem.set_variable_types(enum_variable_types.data(), enum_variable_types.size());
  }

  if (data_model->get_variable_names().size() != 0) {
    op_problem.set_variable_names(data_model->get_variable_names());
  }

  if (data_model->get_row_names().size() != 0) {
    op_problem.set_row_names(data_model->get_row_names());
  }

  return op_problem;
}

/**
 * @brief Wrapper for linear_programming to expose the API to cython
 *
 * @param problem_interface Problem interface (GPU or CPU backend)
 * @param solver_settings PDLP solver settings object
 * @return lp_solution_interface_t pointer (raw pointer, caller owns)
 */
cuopt::linear_programming::lp_solution_interface_t<int, double>* call_solve_lp(
  cuopt::linear_programming::optimization_problem_interface_t<int, double>* problem_interface,
  cuopt::linear_programming::pdlp_solver_settings_t<int, double>& solver_settings,
  bool is_batch_mode)
{
  raft::common::nvtx::range fun_scope("Call Solve LP");
  cuopt_expects(
    problem_interface->get_problem_category() == cuopt::linear_programming::problem_category_t::LP,
    error_type_t::ValidationError,
    "LP solve cannot be called on a MIP problem!");
  const bool problem_checking     = true;
  const bool use_pdlp_solver_mode = true;

  // Solve returns unique_ptr<lp_solution_interface_t>
  auto solution_interface = cuopt::linear_programming::solve_lp(
    problem_interface, solver_settings, problem_checking, use_pdlp_solver_mode, is_batch_mode);

  // Return raw pointer (Python wrapper will own and manage lifecycle)
  return solution_interface.release();
}

/**
 * @brief Wrapper for linear_programming to expose the API to cython
 *
 * @param problem_interface Problem interface (GPU or CPU backend)
 * @param solver_settings MIP solver settings object
 * @return mip_solution_interface_t pointer (raw pointer, caller owns)
 */
cuopt::linear_programming::mip_solution_interface_t<int, double>* call_solve_mip(
  cuopt::linear_programming::optimization_problem_interface_t<int, double>* problem_interface,
  cuopt::linear_programming::mip_solver_settings_t<int, double>& solver_settings)
{
  raft::common::nvtx::range fun_scope("Call Solve MIP");
  cuopt_expects((problem_interface->get_problem_category() ==
                 cuopt::linear_programming::problem_category_t::MIP) or
                  (problem_interface->get_problem_category() ==
                   cuopt::linear_programming::problem_category_t::IP),
                error_type_t::ValidationError,
                "MIP solve cannot be called on an LP problem!");

  // Solve returns unique_ptr<mip_solution_interface_t>
  auto solution_interface =
    cuopt::linear_programming::solve_mip(problem_interface, solver_settings);

  // Return raw pointer (Python wrapper will own and manage lifecycle)
  return solution_interface.release();
}

std::unique_ptr<solver_ret_t> call_solve(
  cuopt::mps_parser::data_model_view_t<int, double>* data_model,
  cuopt::linear_programming::solver_settings_t<int, double>* solver_settings,
  unsigned int flags,
  bool is_batch_mode)
{
  raft::common::nvtx::range fun_scope("Call Solve");

  // Determine backend type based on environment variables
  auto backend_type = cuopt::linear_programming::get_backend_type();

  solver_ret_t response;

  // Create problem instance and CUDA resources based on backend type
  if (backend_type == cuopt::linear_programming::problem_backend_t::GPU) {
    // GPU backend: Create CUDA resources and GPU problem
    rmm::cuda_stream stream(static_cast<rmm::cuda_stream::flags>(flags));
    const raft::handle_t handle_{stream};

    auto gpu_problem = cuopt::linear_programming::gpu_optimization_problem_t<int, double>(&handle_);
    cuopt::linear_programming::populate_from_data_model_view(&gpu_problem, data_model);

    // Handle warmstart data
    if (solver_settings->get_pdlp_warm_start_data_view()
          .last_restart_duality_gap_dual_solution_.data() != nullptr) {
      cuopt::linear_programming::pdlp_warm_start_data_t<int, double> pdlp_warm_start_data(
        solver_settings->get_pdlp_warm_start_data_view(), handle_.get_stream());
      solver_settings->get_pdlp_settings().set_pdlp_warm_start_data(pdlp_warm_start_data);
    }

    // Call appropriate solve function
    if (gpu_problem.get_problem_category() == linear_programming::problem_category_t::LP) {
      response.lp_solution =
        call_solve_lp(&gpu_problem, solver_settings->get_pdlp_settings(), is_batch_mode);
      response.mip_solution = nullptr;
      response.problem_type = linear_programming::problem_category_t::LP;
    } else {
      response.mip_solution = call_solve_mip(&gpu_problem, solver_settings->get_mip_settings());
      response.lp_solution  = nullptr;
      response.problem_type = linear_programming::problem_category_t::MIP;
    }

    // CRITICAL: Transfer solution's device memory to persistent stream before local stream is
    // destroyed. The solution contains device_uvectors allocated on our local stream. When this
    // function returns, the local stream will be destroyed, but the solution object persists in
    // Python. We must transfer ownership to a persistent stream to avoid segfaults during cleanup.
    stream.synchronize();  // Ensure all operations on local stream are complete

    // Transfer solution device memory to per-thread default stream
    if (response.lp_solution) {
      auto* gpu_lp_sol = dynamic_cast<cuopt::linear_programming::gpu_lp_solution_t<int, double>*>(
        response.lp_solution);
      if (gpu_lp_sol) {
        // Access the underlying optimization_problem_solution_t and transfer its streams
        auto& sol = gpu_lp_sol->get_solution();
        sol.get_primal_solution().set_stream(rmm::cuda_stream_per_thread);
        sol.get_dual_solution().set_stream(rmm::cuda_stream_per_thread);
        sol.get_reduced_cost().set_stream(rmm::cuda_stream_per_thread);

        // Transfer warmstart data streams if present
        auto& ws_data = sol.get_pdlp_warm_start_data();
        if (ws_data.current_primal_solution_.size() > 0) {
          ws_data.current_primal_solution_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.current_dual_solution_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.initial_primal_average_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.initial_dual_average_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.current_ATY_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.sum_primal_solutions_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.sum_dual_solutions_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.last_restart_duality_gap_primal_solution_.set_stream(rmm::cuda_stream_per_thread);
          ws_data.last_restart_duality_gap_dual_solution_.set_stream(rmm::cuda_stream_per_thread);
        }
      }
    }

    if (response.mip_solution) {
      auto* gpu_mip_sol = dynamic_cast<cuopt::linear_programming::gpu_mip_solution_t<int, double>*>(
        response.mip_solution);
      if (gpu_mip_sol) {
        // Access the underlying mip_solution_t and transfer its streams
        auto& sol = gpu_mip_sol->get_solution();
        sol.get_solution().set_stream(rmm::cuda_stream_per_thread);
      }
    }

    // Reset warmstart data streams in solver_settings to per-thread default before destroying our
    // local stream. The warmstart data was created using our stream and its uvectors are associated
    // with it.
    auto& warmstart_data = solver_settings->get_pdlp_settings().get_pdlp_warm_start_data();
    if (warmstart_data.current_primal_solution_.size() > 0) {
      warmstart_data.current_primal_solution_.set_stream(rmm::cuda_stream_per_thread);
      warmstart_data.current_dual_solution_.set_stream(rmm::cuda_stream_per_thread);
      warmstart_data.initial_primal_average_.set_stream(rmm::cuda_stream_per_thread);
      warmstart_data.initial_dual_average_.set_stream(rmm::cuda_stream_per_thread);
      warmstart_data.current_ATY_.set_stream(rmm::cuda_stream_per_thread);
      warmstart_data.sum_primal_solutions_.set_stream(rmm::cuda_stream_per_thread);
      warmstart_data.sum_dual_solutions_.set_stream(rmm::cuda_stream_per_thread);
      warmstart_data.last_restart_duality_gap_primal_solution_.set_stream(
        rmm::cuda_stream_per_thread);
      warmstart_data.last_restart_duality_gap_dual_solution_.set_stream(
        rmm::cuda_stream_per_thread);
    }
  } else {
    // CPU backend: No CUDA resources, create CPU problem for remote execution
    auto cpu_problem = cuopt::linear_programming::cpu_optimization_problem_t<int, double>(nullptr);
    cuopt::linear_programming::populate_from_data_model_view(&cpu_problem, data_model);

    // Call appropriate solve function
    if (cpu_problem.get_problem_category() == linear_programming::problem_category_t::LP) {
      response.lp_solution =
        call_solve_lp(&cpu_problem, solver_settings->get_pdlp_settings(), is_batch_mode);
      response.mip_solution = nullptr;
      response.problem_type = linear_programming::problem_category_t::LP;
    } else {
      response.mip_solution = call_solve_mip(&cpu_problem, solver_settings->get_mip_settings());
      response.lp_solution  = nullptr;
      response.problem_type = linear_programming::problem_category_t::MIP;
    }
  }

  return std::make_unique<solver_ret_t>(std::move(response));
}

static int compute_max_thread(
  const std::vector<cuopt::mps_parser::data_model_view_t<int, double>*>& data_models)
{
  constexpr std::size_t max_total = 4;

  // Computing on the total_mem as LP is suppose to run on a single exclusive GPU
  std::size_t free_mem, total_mem;
  RAFT_CUDA_TRY(cudaMemGetInfo(&free_mem, &total_mem));

  // Approximate the necessary memory for each problem
  std::size_t needed_memory = 0;
  for (const auto data_model : data_models) {
    const int nb_variables   = data_model->get_objective_coefficients().size();
    const int nb_constraints = data_model->get_constraint_bounds().size();
    // Currently we roughly need 8 times more memory than the size of each structure in the
    // problem representation
    needed_memory += ((nb_variables * 3 * sizeof(double)) + (nb_constraints * 3 * sizeof(double)) +
                      data_model->get_constraint_matrix_values().size() * sizeof(double) +
                      data_model->get_constraint_matrix_indices().size() * sizeof(int) +
                      data_model->get_constraint_matrix_offsets().size() * sizeof(int)) *
                     8;
  }

  const int res = std::min(max_total, std::min(total_mem / needed_memory, data_models.size()));
  cuopt_expects(
    res > 0, error_type_t::RuntimeError, "Problems too big to be solved in batch mode.");
  // A front end mecanism should prevent users to pick one or more problems so large that this
  // would return 0
  return res;
}

std::pair<std::vector<std::unique_ptr<solver_ret_t>>, double> call_batch_solve(
  std::vector<cuopt::mps_parser::data_model_view_t<int, double>*> data_models,
  cuopt::linear_programming::solver_settings_t<int, double>* solver_settings)
{
  raft::common::nvtx::range fun_scope("Call batch solve");

  const std::size_t size = data_models.size();

  std::vector<std::unique_ptr<solver_ret_t>> list(size);

  auto start_solver = std::chrono::high_resolution_clock::now();

  // Limit parallelism as too much stream overlap gets too slow
  const int max_thread = compute_max_thread(data_models);

  if (solver_settings->get_parameter<int>(CUOPT_METHOD) == CUOPT_METHOD_CONCURRENT) {
    CUOPT_LOG_INFO("Concurrent mode not supported for batch solve. Using PDLP instead. ");
    CUOPT_LOG_INFO(
      "Set the CUOPT_METHOD parameter to CUOPT_METHOD_PDLP or CUOPT_METHOD_DUAL_SIMPLEX to avoid "
      "this warning.");
    solver_settings->set_parameter(CUOPT_METHOD, CUOPT_METHOD_PDLP);
  }

  const bool is_batch_mode = true;

#pragma omp parallel for num_threads(max_thread)
  for (std::size_t i = 0; i < size; ++i)
    list[i] = call_solve(data_models[i], solver_settings, cudaStreamNonBlocking, is_batch_mode);

  auto end      = std::chrono::high_resolution_clock::now();
  auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start_solver);

  return {std::move(list), duration.count() / 1000.0};
}

}  // namespace cython
}  // namespace cuopt
